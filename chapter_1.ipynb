{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6ce096",
   "metadata": {},
   "source": [
    "# Overview of MEG/EEG analysis with MNE-Python\n",
    "\n",
    "## 1. Loading data\n",
    "\n",
    "We can use the `mne.io.read_raw_fif()` function to load data. *MNE-Python data structures* are based around the *FIF* file format from Neuromag, but there are also *other reader functions* for a [wide variety of other data formats](https://mne.tools/stable/documentation/implementation.html#data-formats).\n",
    "<br />\n",
    "*MNE-Python* can also download and manage variety of [public available datasets via its interfaces](https://mne.tools/stable/documentation/datasets.html#datasets).\n",
    "<br />\n",
    "<br />\n",
    "Let's load one of the example dataset, \"[Samples](https://mne.tools/stable/documentation/datasets.html#sample-dataset)\", which contains EEG and MEG data from one subject performing an audiovisual experiment, along with structural MRI scans for that subject.\n",
    "<br />\n",
    "The `mne.datasets.sample.data_path()` interface function will automatically download the dataset if it isn’t found in one of the expected locations or we can specify where to download the data by passing the `path=` parameter. The interface function then return the directory path to the dataset.\n",
    "> Note: The \"Samples\" dataset contains two datasets `sample_audvis_raw.fif` which  is an unfiltered version and `sample_audvis_filt-0-40_raw.fif` which is a filtered and downsampled version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39586fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import mne\n",
    "mne.viz.set_3d_backend(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data \n",
    "sample_data_path = mne.datasets.sample.data_path(\n",
    "    path=\"./\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\\nsPath to the dataset: {sample_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_version = \"sample_audvis_raw.fif\"\n",
    "sample_audvis_raw_path = os.path.join(\"MEG/sample/\", sample_version)\n",
    "sample_data_path = os.path.join(sample_data_path, sample_audvis_raw_path)\n",
    "\n",
    "print(f\"Loading {sample_version} dataset from the path:\\n\\t{sample_data_path}\\n\\n\")\n",
    "sample_raw = mne.io.read_raw_fif(sample_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1c7fa",
   "metadata": {},
   "source": [
    "We can see, by deafult, `read_raw_fif()` displays some info about the dataset it has loaded. Howerver, we can also get some basic details of a [`Raw` dataset object](https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw) by printing it or by calling its `.info` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01acb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_raw)\n",
    "print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "print(sample_raw.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf9ec6",
   "metadata": {},
   "source": [
    "The `Raw` object also comes with several built-in plotting method to visualise the sensors readings:\n",
    "- [`compute_psd()`](https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.compute_psd) -  to compute the **power spectral density (PSD)** for each sensor type. *Power spectral density (PSD)* is a way to describe how the power of a signal is distributed across different frequencies.\n",
    "- [`plot()`](https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.plot) - to plot the raw sensor traces or the `compute_psd()` data. In interactive Python sessions, `plot()` is interactive and allows scrolling, scaling, bad channel marking, annotations, projector toggling, etc.\n",
    "\n",
    "> Note: Here in the PSD plot, we’ll only plot frequencies below 50 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSD plot\n",
    "sample_raw.compute_psd(fmax=50).plot(picks=\"data\", exclude=\"bads\", amplitude=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot raw sensor readings\n",
    "sample_raw.plot(duration=5, n_channels=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3857e055",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Now, raw brain/neuron signals (EEG/MEG/iEEG/LFP) are noisy and includes suppress environmental noise (e.g., 50/60 Hz mains), sensor drift, and muscle/eye/heart artifacts, and so the main neural activity readings fades out. Filtering and preprocessing are what turn “raw, messy voltages” into data you can trust for analysis and modeling.\n",
    "<br />\n",
    "MNE-python provides [`mne.preprocessing`](https://mne.tools/stable/api/preprocessing.html#module-mne.preprocessing) and [`mne.filter`](https://mne.tools/stable/api/preprocessing.html#module-mne.filter) submodules to support a variety of preprocessing approaches and techniques (maxwell filtering, signal-space projection, independent components analysis, filtering, downsampling, etc)\n",
    "<br />\n",
    "<br />\n",
    "One way to clean up the sensor readings is by using **independent components analysis (ICA)**. \n",
    "<br />\n",
    "Think of your sensors like several microphones in a noisy room: they record a jumble of sounds at once.  *ICA* tries to actually separate the voices so each track is mostly one thing — eye blinks, heartbeat, or a brain rhythm. Then you can mute the blink/heartbeat tracks and keep the brain track, making the data much cleaner. We filter first (remove slow drift and mains hum) so *ICA* sees clearer patterns and doesn’t waste effort on obvious noise. \n",
    "> Note: ICA is bit different than PCA (Principal Component Analysis), which is like pointing the mics to where the sound is strongest—useful for shrinking the data, but the voices are still mixed.\n",
    "\n",
    "Luckily, MNE-Python has `mne.preprocessing.ICA`. Let's use it to clean up our data by performing and for brevity we’ll skip the steps that helped us determined which components best capture the artifacts and that we want to remove them. \n",
    "<br />\n",
    "So, once we’re confident about which component(s) we want to remove, we pass them as the `exclude` parameter and then apply the *ICA* to the raw signal. The [`apply()`](https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.apply) method requires the raw data to be loaded into memory (by default it’s only read from disk as-needed), so we’ll use [`load_data()`](https://mne.tools/stable/generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.apply) first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the ICA\n",
    "ica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\n",
    "# fit our sample dataset\n",
    "ica.fit(sample_raw)\n",
    "\n",
    "# pick the properties we want to exclude,\n",
    "# details on how we picked these are omitted here\n",
    "ica.exclude = [1, 2] \n",
    "# plot those properties\n",
    "ica.plot_properties(sample_raw, picks=ica.exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537a2d4",
   "metadata": {},
   "source": [
    "We’ll also make a copy of the `Raw` object before loading the data on memory and applying the *ICA* so we can compare the signal before and after artifact removal side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sample_raw = sample_raw.copy()\n",
    "\n",
    "# load the data\n",
    "sample_raw.load_data()\n",
    "# apply ICA\n",
    "ica.apply(sample_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some frontal channels to clearly illustrate the artifact removal\n",
    "chs = [\n",
    "    \"MEG 0111\",\n",
    "    \"MEG 0121\",\n",
    "    \"MEG 0131\",\n",
    "    \"MEG 0211\",\n",
    "    \"MEG 0221\",\n",
    "    \"MEG 0231\",\n",
    "    \"MEG 0311\",\n",
    "    \"MEG 0321\",\n",
    "    \"MEG 0331\",\n",
    "    \"MEG 1511\",\n",
    "    \"MEG 1521\",\n",
    "    \"MEG 1531\",\n",
    "    \"EEG 001\",\n",
    "    \"EEG 002\",\n",
    "    \"EEG 003\",\n",
    "    \"EEG 004\",\n",
    "    \"EEG 005\",\n",
    "    \"EEG 006\",\n",
    "    \"EEG 007\",\n",
    "    \"EEG 008\",\n",
    "]\n",
    "\n",
    "# channels index\n",
    "chan_idxs = [sample_raw.ch_names.index(ch) for ch in chs]\n",
    "# plot the readings\n",
    "orig_sample_raw.plot(order=chan_idxs, start=12, duration=4)\n",
    "sample_raw.plot(order=chan_idxs, start=12, duration=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe50a3",
   "metadata": {},
   "source": [
    "## Detecting experimental events\n",
    "\n",
    "The sample dataset includes several *“STIM” channels* that recorded electrical signals sent from the *stimulus delivery computer* (as brief DC shifts / squarewave pulses). These pulses (often called *“triggers”*) are used in this dataset to mark experimental events: *stimulus onset*, *stimulus type*, and *participant response (button press)*.\n",
    "<br/>\n",
    "The individual STIM channels are combined onto a single channel, in such a way that voltage levels on that channel can be unambiguously decoded as a particular event type.\n",
    "<br />\n",
    "On older *Neuromag systems* (such as that used to record the sample data) this summation channel was called `STI 014`, so we can pass that channel name to the [`mne.find_events()`](https://mne.tools/stable/generated/mne.find_events.html#mne.find_events) function to recover the timing and identity of the stimulus events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "stims_events = mne.find_events(sample_raw, stim_channel=\"STI 014\")\n",
    "# show the first 5\n",
    "print(\"=================\")\n",
    "print(stims_events[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90891181",
   "metadata": {},
   "source": [
    "The resulting events array above is an ordinary 3-column *NumPy array*, with *sample number* in the first column and *integer event ID* in the last column; the middle column is usually ignored.\n",
    "<br />\n",
    "Rather than keeping track of integer event IDs, we can provide an event dictionary that maps the integer IDs to experimental conditions or events. In this dataset, the mapping looks like this:\n",
    "|Event ID|Condition|\n",
    "|--------|---------|\n",
    "| 1 | auditory stimulus (tone) to the left ear |\n",
    "| 2 | auditory stimulus (tone) to the right ear |\n",
    "| 3 | visual stimulus (checkerboard) to the left visual field |\n",
    "| 4 | visual stimulus (checkerboard) to the right visual field |\n",
    "| 5 | smiley face (catch trial) |\n",
    "| 32 | subject button press |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_event_dict = {\n",
    "    \"auditory/left\": 1,\n",
    "    \"auditory/right\": 2,\n",
    "    \"visual/left\": 3,\n",
    "    \"visual/right\": 4,\n",
    "    \"smiley\": 5,\n",
    "    \"buttonpress\": 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61414671",
   "metadata": {},
   "source": [
    "Let's now plot these events using `mne.viz.plot_events()` function to visualise the distribution of events across the duration of the recording.\n",
    "> Note: Here we will also make use of the Info attribute to get the sampling frequency of the recording (so our x-axis will be in seconds instead of in samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00dcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mne.viz.plot_events(\n",
    "    stims_events, event_id=stim_event_dict, sfreq=sample_raw.info[\"sfreq\"], first_samp=sample_raw.first_samp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0fff7b",
   "metadata": {},
   "source": [
    "For paradigms that are not event-related (e.g., analysis of resting-state data), you can extract regularly spaced (possibly overlapping) spans of data by creating events using `mne.make_fixed_length_events()` and then proceeding with **epoching**.\n",
    "\n",
    "> Note: Resting-state means recording brain activity while the person (or animal) isn’t doing any specific task—usually just sitting quietly with eyes open (fixating) or eyes closed, relaxed and still. In this condition, the signals reflect the brain’s spontaneous rhythms and how regions naturally interact (e.g., the alpha rhythm around 8–12 Hz often grows with eyes closed). Researchers use resting-state data as a baseline, to study networks/connectivity, and to compare groups or conditions without the confounds of task performance.\n",
    "\n",
    "### Epoching continuous data\n",
    "\n",
    "An epoch is just a short slice of your long, continuous recording taken around a meaningful event—like “from 200 ms before a beep to 800 ms after it.” We epoch continuous data so we can line up many repeats of the same event, throw out slices with big artifacts (blinks, movement), do baseline correction using the pre-event period, and then average or compare conditions. This boosts the true, time-locked brain response while unrelated noise cancels out, makes statistics straightforward, and keeps data sizes manageable. For resting-state or very slow trends you might stay continuous, but for event-related questions, epoching is the standard way to get clean, comparable trials.\n",
    "<br />\n",
    "Now because each epoch is built from many channels at once, a single big artifact on any one channel can ruin the whole trial and bias your averages, stats, covariance, or decoding. Reject criteria are simple, objective limits (e.g., “drop the epoch if any channel’s peak-to-peak is too large, or if a channel goes flat/clipped”) that automatically exclude bad trials so only clean, comparable data remain. They’re set per channel type because sensors have different units and typical amplitudes (EEG vs MEG vs EOG), and using type-appropriate thresholds keeps you from over- or under-rejecting. Net effect: fewer outliers, better SNR, and more trustworthy results across all channels.\n",
    "<br />\n",
    "<br />\n",
    "In MNE-Python, the `Raw` object and the *events array* are the bare minimum needed to create an [`Epochs`](https://mne.tools/stable/generated/mne.Epochs.html#mne.Epochs) object, which we create with the *`Epochs` class constructor*.\n",
    "<br />\n",
    "In our example, we’ll also specify some data quality constraints: we’ll reject any *epoch* where *peak-to-peak signal amplitude* is beyond reasonable limits for that *channel type*. This is done with a *rejection dictionary*; you may include or omit thresholds for any of the channel types present in your data.\n",
    "\n",
    "> Note: The values given here are reasonable for this particular dataset, but may need to be adapted for different hardware or recording conditions. For a more automated approach, consider using the [autoreject package](http://autoreject.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0776c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_criteria = dict(\n",
    "    mag=4000e-15, # 400 ft\n",
    "    grad=1000e-13, # 4000 ft/cm\n",
    "    eeg=150e-6, # 1500 µV\n",
    "    eog=250e-6 # 150 µV\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a8d5d",
   "metadata": {},
   "source": [
    "Now, let's create an `Epochs` object by passing the event dictionary as the `event_id` parameter (so we can work with easy-to-pool event labels instead of the integer event IDs), and specify `tmin` and `tmax` params (the time relative to each event at which to start and end each epoch).\n",
    "\n",
    "> Note: By default `Raw` and `Epochs` data aren’t loaded into memory (they’re accessed from disk only when needed), but here we’ll force loading into memory using the `preload=True` parameter so that we can see the results of the rejection criteria being applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = mne.Epochs(\n",
    "    sample_raw,\n",
    "    stims_events,\n",
    "    event_id=stim_event_dict,\n",
    "    tmin=-0.2,\n",
    "    tmax=0.5,\n",
    "    reject=reject_criteria,\n",
    "    preload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f84a31c",
   "metadata": {},
   "source": [
    "Now in general, you need to balance (equalize) epoch counts so your results aren’t driven by how many trials each subgroup contributes, but by the true effect you care about. If one subgroup (e.g., “left”) has more clean epochs than another (“right”), its average is less noisy and will dominate pooled averages, inflate SNR, skew statistics, and bias decoders simply because it has more data, not stronger neural responses. Equalizing counts (or otherwise weighting) keeps noise levels comparable across groups, avoids class-imbalance bias, makes contrasts fair, and ensures any difference you see reflects the underlying physiology—not unequal trial numbers or artifact-related dropout.\n",
    "<br/>\n",
    "<br />\n",
    "Let's say here, we care about the *auditory* and *visual* groups and we want to compare auditory versus visual responses. So, we’ll pool across *left/right stimulus presentations* and to avoid biasing our signals to the left or right, we’ll use `epochs.equalize_event_counts()` first to randomly sample epochs from each condition to match the number of epochs present in the condition with the fewest good epochs.\n",
    "<br />\n",
    "In simpler words, because when you “pool” left and right trials into a single auditory (or visual) set, you can accidentally overweight whichever side has more clean epochs after rejection (e.g., 120 left vs 80 right). If you average those together as-is, the pooled response is pulled toward the dominant side, mixing true modality effects with left/right lateralization and unequal noise. equalize_event_counts fixes this by randomly downsampling each subgroup so they contribute equally (e.g., 80 left + 80 right), making your auditory–visual comparison fair, keeping signal-to-noise comparable across conditions, and avoiding class-imbalance issues for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93df3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "care_about = [\"auditory/left\", \"auditory/right\", \"visual/left\", \"visual/right\"]\n",
    "epochs.equalize_event_counts(care_about) # it's an in-place operation\n",
    "\n",
    "aud_epochs = epochs[\"auditory\"]\n",
    "vis_epochs = epochs[\"visual\"]\n",
    "\n",
    "# free up memory\n",
    "del sample_raw, epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e121944",
   "metadata": {},
   "source": [
    "`Epochs` object also comes with a built-in plot function `plot_image()`, which shows each epoch as one row of an image map, with color representing signal magnitude; the average evoked response and the sensor location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_epochs.plot_image(picks=[\"MEG 1332\", \"EEG 021\"])\n",
    "vis_epochs.plot_image(picks=[\"MEG 1332\", \"EEG 021\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6feb2b",
   "metadata": {},
   "source": [
    "### Time-frequency analysis\n",
    "\n",
    "Time-frequency analysis is like using a music equalizer that shows which notes (frequencies) are playing at what moments in your brain data. Regular averages only show the overall waveform and can miss short-lived “bursts” or changes that aren’t perfectly time-locked to the event, so they get washed out. By breaking the signal into time and frequency, you can see, for example, a brief alpha dip right after a cue or a beta rebound later—when it happens and how strong it is. This makes it easier to detect transient rhythms, compare conditions fairly (with a baseline), and spot artifacts that sit in particular frequency bands.\n",
    "<br />\n",
    "<br />\n",
    "The `mne.time_frequency` and `Epochs.compute_tfr()` submodule provides implementations of several algorithms to compute *time-frequency representations*, *power spectral density*, and *cross-spectral density*.\n",
    "<br />\n",
    "Here, for example, we’ll compute the induced power at different frequencies and times, using *Morlet wavelets*, for the *auditory epochs*.\n",
    "\n",
    "> Note: On this dataset the result is not especially informative (it just shows the evoked “auditory N100” response); see [here](https://mne.tools/stable/auto_tutorials/time-freq/20_sensors_time_frequency.html#inter-trial-coherence) for a more extended example on a dataset with richer frequency content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2669b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencise = np.arange(\n",
    "    start=7, stop=30, step=3\n",
    ")\n",
    "induced_power = aud_epochs.compute_tfr(\n",
    "    \"morlet\", n_cycles=2, return_itc=False, freqs=frequencise, decim=3, average=True\n",
    ")\n",
    "induced_power.plot([\"MEG 1332\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79109ae9",
   "metadata": {},
   "source": [
    "### Estimating evoked responses\n",
    "*Evoked responses (ERPs/ERFs)* are the brain’s consistent, time-locked reactions to a *stimulus* (like a beep or flash). We estimate them by averaging many aligned *epochs*, which cancels random noise and boosts the tiny, repeatable signal, so we can clearly see when the brain responds (latency) and how strongly (amplitude). This cleaner waveform lets us compare conditions or groups (e.g., visual vs auditory, patients vs controls), identify classic components (P1/N1/P300 or M100), do reliable statistics and baseline correction, and even get better source estimates—all because the averaging step greatly improves signal-to-noise for time-locked activity.\n",
    "<br />\n",
    "<br />\n",
    "So for our example, now that we have our conditions in `aud_epochs` and `vis_epochs`, we can get an estimate of evoked responses to *auditory* versus *visual* stimuli by averaging together the epochs in each condition. This is as simple as calling the [`average()`](https://mne.tools/stable/generated/mne.Epochs.html#mne.Epochs.average) method on the `Epochs` object, and then using a function from the [`mne.viz`](https://mne.tools/stable/api/visualization.html#module-mne.viz) module to compare the global field power for each sensor type of the two [`Evoked`](https://mne.tools/stable/generated/mne.Evoked.html#mne.Evoked) objects, using the `.plot_compare_evokeds()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46eb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_evoked = aud_epochs.average()\n",
    "vis_evoked = vis_epochs.average()\n",
    "\n",
    "mne.viz.plot_compare_evokeds(\n",
    "    dict(auditory=aud_evoked, visual=vis_evoked),\n",
    "    legend=\"upper left\",\n",
    "    show_sensors=\"upper right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3821ff",
   "metadata": {},
   "source": [
    "We can also get a more detailed view of each `Evoked` object using other plotting methods such as `.plot_joint` or `.plot_topomap`. \n",
    "<br />\n",
    "Here we’ll examine just the *EEG channels*, and see the classic auditory evoked *N100-P200* pattern over *dorso-frontal electrodes*, then plot scalp topographies at some additional arbitrary times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427738d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_evoked.plot_joint(picks=\"eeg\")\n",
    "aud_evoked.plot_topomap(\n",
    "    times=[0.0, 0.08, 0.1, 0.12, 0.2],\n",
    "    ch_type=\"eeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaae719",
   "metadata": {},
   "source": [
    "`Evoked` objects can also be combined to show contrasts between conditions, using the `mne.combine_evoked()` function. A simple difference can be generated by passing `weights=[1, -1]`. We’ll then plot the difference wave at each sensor using `plot_topo`.\n",
    "<br />\n",
    "In other words, `combine_evoked` with `weights=[1, -1]` makes a difference wave: *Condition A* minus *Condition B* at every time point and sensor. This contrast cancels what both conditions share and highlights what’s truly different — so you can see when the effect happens and where on the scalp/helmet it’s strongest (`plot_topo`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_diff = mne.combine_evoked([aud_evoked, vis_evoked], weights=[1, -1])\n",
    "evoked_diff.pick(picks=\"mag\").plot_topo(color=\"r\", legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6076aa9",
   "metadata": {},
   "source": [
    "### Inverse modeling\n",
    "\n",
    "Inverse modeling answers the “where in the brain did this signal come from?” question. Sensors (EEG/MEG) pick up mixtures of many sources; scalp maps alone can’t tell you the true generators because many different source patterns could produce the same sensor pattern (the problem is ill-posed). Inverse modeling uses a forward/physics head model (anatomy + conductivities) plus constraints/regularization (e.g., minimum-norm, dSPM, sLORETA, beamformers) to estimate activity on the cortex that best explains the sensor data. You get time courses per brain location/ROI, reduced sensor mixing/field-spread issues, better anatomical interpretability, and cleaner group stats and connectivity done in source space rather than at sensors.\n",
    "<br />\n",
    "<br />\n",
    "So, for our example we can estimate the origins of the evoked activity by projecting the sensor data into this subject’s [source space](https://mne.tools/stable/documentation/glossary.html#term-source-space) (a set of points either on the cortical surface or within the cortical volume of that subject, as estimated by structural MRI scans).\n",
    "<br />\n",
    "*MNE-Python* supports lots of ways of doing this (*dynamic statistical parametric mapping, dipole fitting, beamformers*, etc.); here we’ll use **minimum-norm estimation (MNE)** to generate a continuous map of activation constrained to the cortical surface. *MNE* uses a *linear [inverse operator](https://mne.tools/stable/documentation/glossary.html#term-inverse-operator)* to project EEG+MEG sensor measurements into the source space. \n",
    "<br />\n",
    "The *inverse operator* is computed from the [*forward solution*](https://mne.tools/stable/documentation/glossary.html#term-forward-solution) for this subject and an estimate of [the covariance of sensor measurements](https://mne.tools/stable/auto_tutorials/forward/90_compute_covariance.html#tut-compute-covariance).\n",
    "<br />\n",
    "<br />\n",
    "For this tutorial we’ll skip those computational steps and load a pre-computed inverse operator from disk (it’s included with the *sample data*). Because this “inverse problem” is underdetermined/ill-posed (there is no unique solution), here we further constrain the solution by providing a regularization parameter specifying the relative smoothness of the current estimates in terms of a signal-to-noise ratio (where “noise” here is akin to baseline activity level across all of cortex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf359b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the inverse operator\n",
    "inverse_operator_file_path = sample_data_path.split(\"/\")\n",
    "inverse_operator_file_path[-1] = \"sample_audvis-meg-oct-6-meg-inv.fif\"\n",
    "inverse_operator_file_path = \"/\".join(inverse_operator_file_path)\n",
    "\n",
    "assert os.path.exists(inverse_operator_file_path)\n",
    "print(inverse_operator_file_path)\n",
    "\n",
    "# load the inverse operator\n",
    "inv_operator = mne.minimum_norm.read_inverse_operator(inverse_operator_file_path)\n",
    "# set signal-to-noise (SNR) to compute regularization paramter (λ²)\n",
    "snr = 3.0\n",
    "lambda2 = 1.0 / snr**2\n",
    "\n",
    "# generate the source time course (STC)\n",
    "stc = mne.minimum_norm.apply_inverse(\n",
    "    vis_evoked, inv_operator, lambda2=lambda2, method=\"MNE\"  # or dSPM, sLORETA, eLORETA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c65d2",
   "metadata": {},
   "source": [
    "Finally, in order to plot the source estimate on the subject’s cortical surface we’ll also need the path to the sample subject’s structural MRI files (the `subjects_dir`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e37401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to subjects' MRI files\n",
    "subjects_dir = sample_data_path.split(\"/\")\n",
    "subjects_dir = subjects_dir[:2]\n",
    "subjects_dir[-1] = \"subjects\"\n",
    "subjects_dir = \"/\".join(subjects_dir)\n",
    "\n",
    "assert(os.path.exists(subjects_dir))\n",
    "print(subjects_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41590627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the STC\n",
    "stc.plot(\n",
    "    initial_time=0.1, hemi=\"split\", views=[\"lat\", \"med\"], subjects_dir=subjects_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
